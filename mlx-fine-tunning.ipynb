{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d0b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner import TrainingArgs, linear_to_lora_layers, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fcc795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25621e598094d97a5c8eeca03d6618f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mlx-community/Phi-3.5-mini-instruct-4bit\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bd441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "To generate an SQL query to find all users who registered in the last 30 days, you would typically use the current date and subtract 29 days (since the count of days should include the day of registration) and compare it with the registration date in the database. Assuming the table is named `users` and has a column for the registration date, let's call it `registration_date`, and it's stored in a date or timestamp format, the SQL query would look something like this:\n",
      "\n",
      "```sql\n",
      "SELECT *\n",
      "FROM users\n",
      "WHERE registration_date >= CURRENT_DATE - INTERVAL '30 days';\n",
      "```\n",
      "\n",
      "Here's a breakdown of the query:\n",
      "\n",
      "- `SELECT *`: This selects all columns for all rows that match the criteria.\n",
      "- `FROM users`: This indicates that you're querying the `users` table.\n",
      "- `WHERE registration_date >= CURRENT_DATE - INTERVAL '30 days`: This filters the rows to only include those where the `registration_date` is within the last 30 days. `CURRENT_DATE` gets today'ized date, and subtracting an interval of '30\n",
      "==========\n",
      "Prompt: 20 tokens, 113.450 tokens-per-sec\n",
      "Generation: 256 tokens, 36.854 tokens-per-sec\n",
      "Peak memory: 2.397 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"generate an SQL query to find all users who registered in the last 30 days\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b450341",
   "metadata": {},
   "source": [
    "# Creando Adaptador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca69edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d99b8",
   "metadata": {},
   "source": [
    "# Lora config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6b1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"num_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 8,\n",
    "        \"scale\": 20.0,\n",
    "        \"dropout\": 0.0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167a0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(lora_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d27b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_file_path,\n",
    "    iters=200,\n",
    "    steps_per_eval=50,\n",
    "    grad_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b86f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 786432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): QuantizedEmbedding(32064, 3072, group_size=64, bits=4)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=3072, output_dims=9216, bias=False, group_size=64, bits=4)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "        (rope): SuScaledRoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): QuantizedLinear(input_dims=3072, output_dims=16384, bias=False, group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(3072, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): QuantizedLinear(input_dims=3072, output_dims=32064, bias=False, group_size=64, bits=4)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freeze()\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b300e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self) -> None:\n",
    "        self.train_losses: List[Tuple[int, float]] = []\n",
    "        self.val_losses: List[Tuple[int, float]] = []\n",
    "\n",
    "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945ce611",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e2b43",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.tuner.datasets import load_hf_dataset\n",
    "config = { }\n",
    "train_set, val_set, test_set = load_hf_dataset(\n",
    "    data_id=\"mlx-community/wikisql\",\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd0152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 100\n",
      "Validation set size: 100\n",
      "Training set size: 1000\n",
      "test set: {'text': [\"table: 1-10015132-16\\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\\nQ: What is terrence ross' nationality\\nA: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'\", \"table: 1-10015132-16\\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\\nQ: What clu was in toronto 1995-96\\nA: SELECT School/Club Team FROM 1-10015132-16 WHERE Years in Toronto = '1995-96'\"]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"test set: {test_set[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93609708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:39<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 3.253, Val took 40.003s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10: Train loss 3.317, Learning Rate 1.000e-05, It/sec 0.475, Tokens/sec 180.896, Trained Tokens 3812, Peak mem 3.131 GB\n",
      "Iter 20: Train loss 2.594, Learning Rate 1.000e-05, It/sec 0.473, Tokens/sec 175.779, Trained Tokens 7531, Peak mem 3.131 GB\n",
      "Iter 30: Train loss 1.959, Learning Rate 1.000e-05, It/sec 0.407, Tokens/sec 168.812, Trained Tokens 11677, Peak mem 3.131 GB\n",
      "Iter 40: Train loss 1.981, Learning Rate 1.000e-05, It/sec 0.464, Tokens/sec 170.318, Trained Tokens 15347, Peak mem 3.131 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:56<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Val loss 1.654, Val took 56.184s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Train loss 1.563, Learning Rate 1.000e-05, It/sec 0.412, Tokens/sec 162.975, Trained Tokens 19301, Peak mem 3.199 GB\n",
      "Iter 60: Train loss 1.541, Learning Rate 1.000e-05, It/sec 0.413, Tokens/sec 165.119, Trained Tokens 23302, Peak mem 3.199 GB\n",
      "Iter 70: Train loss 1.684, Learning Rate 1.000e-05, It/sec 0.374, Tokens/sec 146.850, Trained Tokens 27233, Peak mem 3.199 GB\n",
      "Iter 80: Train loss 1.457, Learning Rate 1.000e-05, It/sec 0.292, Tokens/sec 118.472, Trained Tokens 31297, Peak mem 3.199 GB\n",
      "Iter 90: Train loss 1.465, Learning Rate 1.000e-05, It/sec 0.322, Tokens/sec 130.507, Trained Tokens 35346, Peak mem 3.199 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:49<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 1.419, Val took 49.254s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Train loss 1.346, Learning Rate 1.000e-05, It/sec 0.380, Tokens/sec 150.027, Trained Tokens 39293, Peak mem 3.199 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.546, Learning Rate 1.000e-05, It/sec 0.408, Tokens/sec 159.853, Trained Tokens 43210, Peak mem 3.199 GB\n",
      "Iter 120: Train loss 1.345, Learning Rate 1.000e-05, It/sec 0.427, Tokens/sec 172.446, Trained Tokens 47253, Peak mem 3.202 GB\n",
      "Iter 130: Train loss 1.356, Learning Rate 1.000e-05, It/sec 0.442, Tokens/sec 178.428, Trained Tokens 51291, Peak mem 3.202 GB\n",
      "Iter 140: Train loss 1.373, Learning Rate 1.000e-05, It/sec 0.461, Tokens/sec 166.150, Trained Tokens 54897, Peak mem 3.202 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:47<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150: Val loss 1.345, Val took 48.049s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150: Train loss 1.393, Learning Rate 1.000e-05, It/sec 0.420, Tokens/sec 163.223, Trained Tokens 58785, Peak mem 3.202 GB\n",
      "Iter 160: Train loss 1.287, Learning Rate 1.000e-05, It/sec 0.380, Tokens/sec 150.868, Trained Tokens 62750, Peak mem 3.202 GB\n",
      "Iter 170: Train loss 1.254, Learning Rate 1.000e-05, It/sec 0.342, Tokens/sec 132.821, Trained Tokens 66633, Peak mem 3.202 GB\n",
      "Iter 180: Train loss 1.169, Learning Rate 1.000e-05, It/sec 0.263, Tokens/sec 106.740, Trained Tokens 70684, Peak mem 3.208 GB\n",
      "Iter 190: Train loss 1.383, Learning Rate 1.000e-05, It/sec 0.271, Tokens/sec 106.087, Trained Tokens 74593, Peak mem 3.208 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [01:07<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 1.314, Val took 67.973s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Train loss 1.287, Learning Rate 1.000e-05, It/sec 0.307, Tokens/sec 112.211, Trained Tokens 78253, Peak mem 3.208 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Saved final weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.tuner.datasets import CacheDataset\n",
    "\n",
    "train_dataset = CacheDataset(train_set)\n",
    "val_dataset = CacheDataset(val_set)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optim.Adam(learning_rate=1e-5),\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    args=training_args,\n",
    "    training_callback=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f932b34",
   "metadata": {},
   "source": [
    "## Fusionar modelo base con adaptador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255d5151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "! python -m mlx_lm fuse  --model ./phi3-mlx --adapter-path ./adapters --save-path ./new_phi3-mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838e4911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install huggingface_hub\n",
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9616483",
   "metadata": {},
   "source": [
    "# Subir modelo a HF\n",
    "Utilizando el API de HF se sube el modelo a deimagjas/Phi-3.5-mini-instruct-4bit-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a7a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7013cb6738f74a6faf23eb023602643e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ef97ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e63d7ce1e646b9b59799f18594baf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6673ba2a1c946ebac2992c464cc8f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d0337eac8b4666ba7c9e85dc99e4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ning/new_phi3-mlx/model.safetensors:   6%|6         |  130MB / 2.15GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/deimagjas/Phi-3.5-mini-instruct-4bit-sft/commit/760b8fb8e80a39161f9f1aefb8e147e19adeba46', commit_message='Upload folder using huggingface_hub', commit_description='', oid='760b8fb8e80a39161f9f1aefb8e147e19adeba46', pr_url=None, repo_url=RepoUrl('https://huggingface.co/deimagjas/Phi-3.5-mini-instruct-4bit-sft', endpoint='https://huggingface.co', repo_type='model', repo_id='deimagjas/Phi-3.5-mini-instruct-4bit-sft'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"deimagjas/Phi-3.5-mini-instruct-4bit-sft\"\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=\"./new_phi3-mlx\",\n",
    "    repo_id=repo_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c7625",
   "metadata": {},
   "source": [
    "## Test HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e405bb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28ba0c006104996b15316300d29f6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca30a2d0abb4c3b839beb9164f9bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"deimagjas/Phi-3.5-mini-instruct-4bit-sft\"\n",
    "model_sft, tokenizer_sft = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eeb3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "To generate an SQL query that finds all users who have registered in the last 30 days, you'll need to use the `CURRENT_DATE` or a function equivalent in your specific SQL dialect (like `GETDATE()` in SQL Server, `CURRENT_DATE` in MySQL, or `SYSDATE()` in Oracle) to get the current date and then subtract 30 days to find the date 30 days ago. Assuming you have a table named `users` with a `registration_date` column that stores the registration date, and the date is stored in a standard format (like `YYYY-MM-DD`), the query would look something like this:\n",
      "\n",
      "\n",
      "```sql\n",
      "\n",
      "SELECT * FROM users\n",
      "WHERE registration_date >= CURRENT_DATE - INTERVAL '30 days'\n",
      "\n",
      "```sql\n",
      "\n",
      "-- For MySQL\n",
      "SELECT * FROM users\n",
      "WHERE registration_date >= CURRENT_DATE - INTERVAL 30 DAY;\n",
      "\n",
      "```\n",
      "\n",
      "Or\n",
      "\n",
      "```sql\n",
      "\n",
      "-- For PostgreSQL\n",
      "SELECT * FROM users\n",
      "WHERE registration_date >= CURRENT_DATE - INTERVAL '30 days';\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "==========\n",
      "Prompt: 20 tokens, 73.271 tokens-per-sec\n",
      "Generation: 256 tokens, 36.304 tokens-per-sec\n",
      "Peak memory: 2.467 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"generate an SQL query to find all users who registered in the last 30 days\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer_sft.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = generate(model_sft, tokenizer_sft, prompt=prompt, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
