{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd86a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "#!%pip install \"unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\"\n",
    "#   Install Unsloth library\n",
    "! pip install unsloth --upgrade --no-cache-dir\n",
    "\n",
    "# Install Flash Attention 2 for softcapping support\n",
    "import torch\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    ! pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n",
    "!%pip install trl==0.8.5\n",
    "#!%pip install --no-deps xformers peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99289925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8fc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!\n",
    "# Get LAION dataset\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", #or choose any model\n",
    "\n",
    "] # More models at https://huggingface.co/unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56314fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4-bit quantization. False = 16-bit LoRA.\n",
    "    load_in_8bit = False, # 8-bit quantization\n",
    "    load_in_16bit = False, # [NEW!] 16-bit LoRA\n",
    "    full_finetuning = False, # Use for full fine-tuning.\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ced9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28de19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        max_seq_length = max_seq_length,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Go to https://docs.unsloth.ai for advanced tips like\n",
    "# (1) Saving to GGUF / merging to 16bit for vLLM\n",
    "# (2) Continued training from a saved LoRA adapter\n",
    "# (3) Adding an evaluation loop / OOMs\n",
    "# (4) Customized chat templates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
