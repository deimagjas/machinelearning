{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ce18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La variable de entorno HF_HOME se ha establecido en: /Users/deimagjas/qubits.cloud/machinelearning/gemma3-finetunning/models_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "project_root = pathlib.Path.cwd()\n",
    "\n",
    "cache_dir = project_root / \"models_cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "os.environ['HF_HOME'] = str(cache_dir)\n",
    "print(f\"La variable de entorno HF_HOME se ha establecido en: {os.environ['HF_HOME']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d0b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner import TrainingArgs, linear_to_lora_layers, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5647f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlx-lm\n",
      "Version: 0.28.2\n",
      "Location: /Users/deimagjas/qubits.cloud/machinelearning/gemma3-finetunning/.venv/lib/python3.13/site-packages\n",
      "Requires: jinja2, mlx, numpy, protobuf, pyyaml, transformers\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show mlx_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a7a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa3733f667b4c779a419c5003ff92c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e1a5a",
   "metadata": {},
   "source": [
    "## Carga de modelo desde HF\n",
    "\n",
    "La razón por la que el código funciona con google/gemma-3-270m-it pero no con google/gemma-3-270m se debe a la diferencia entre los\n",
    "  dos tipos de modelos:\n",
    "\n",
    "   1. `google/gemma-3-270m-it`: El sufijo \"-it\" significa \"Instruction Tuned\" (ajustado para instrucciones). Este modelo ha sido\n",
    "      específicamente entrenado para entender y seguir instrucciones en un formato de chat o de pregunta-respuesta. Su tokenizador\n",
    "      incluye una \"plantilla de chat\" (chat template) que formatea la entrada de manera que el modelo la entienda.\n",
    "\n",
    "   2. `google/gemma-3-270m`: Este es el modelo base. Es un modelo de lenguaje pre-entrenado que es bueno para predecir la siguiente\n",
    "      palabra en un texto, pero no ha sido ajustado para seguir instrucciones o para conversar. Su tokenizador no tiene una plantilla de\n",
    "      chat predeterminada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fcc795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbfdfa7ebd54399a2aec1b2aaf063f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bb31d969534a03bae90d4f55cf6b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69471928b2d3422b9808dcba8668679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5754984607473b9137b0d4b2ae8a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085922e0ec1246dba412cc31b59b0771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6138986b734846efbb9f94e974f249df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67595f763c6240a48b72da3536511d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa495e481764f25b90eda0ba62378c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d84a54e239476f878c8cb0750db9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1025c4640ab74cbe8d0a23e180c32de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "\n",
    "sampler = make_sampler(temp=0.2)\n",
    "model_path = \"google/functiongemma-270m-it\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bd441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "I can assist with a wide range of tasks related to information and assistance. My capabilities include:\n",
      "*   Providing knowledge and information\n",
      "*   Translating languages\n",
      "*   Writing\n",
      "*   Brainstorming ideas\n",
      "*   Scheduling\n",
      "*   Managing tasks\n",
      "\n",
      "I am ready to assist with whatever you need.<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<start_function_call> model비로 답변할 수 있습니다.<end_of_turn>\n",
      "<start_function_call> unknown<end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown<escape><end_of_turn>\n",
      "<start_function_call> unknown\n",
      "==========\n",
      "Prompt: 17 tokens, 42.749 tokens-per-sec\n",
      "Generation: 256 tokens, 400.747 tokens-per-sec\n",
      "Peak memory: 0.551 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"create a list with what you can do\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "        sampler=sampler         \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bb9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82f48e382d04748884e1d2db93baeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using dtype: float16\n",
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.502 bits per weight.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdc62dbe29248029eb135773c65f19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import convert\n",
    "\n",
    "repo = model_path \n",
    "\n",
    "convert(\n",
    "        repo,\n",
    "        quantize=True,\n",
    "        dtype=\"float16\",\n",
    "        q_bits=4,\n",
    "        mlx_path=\"./gemma3f-mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4284343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "I'm a specialized AI model created by the creators of 'Zero.AI'. My purpose is to provide helpful and informative responses within a specific topic. I cannot directly list all possible capabilities. My purpose is to use this capability for a specific task.<end_of_turn>\n",
      "I am a specialized AI model created by the creators of 'Zero.AI'. My purpose is to provide helpful and informative responses within a specific topic. I cannot directly list all possible capabilities.<end_of_turn><end_of_turn>\n",
      "I cannot assist with this request. My purpose is to use this capability for a specific task.<end_of_turn>\n",
      "<end_of_turn>\n",
      "I cannot provide a list of possible capabilities. My purpose is to use this capability for a specific task.<end_of_turn>\n",
      "<end_of_turn>\n",
      "I am a specialized AI model created by the creators of 'Zero.AI'. My purpose is to provide helpful and informative responses within a specific topic. I cannot directly list all possible capabilities.<end_of_turn><end_of_turn>\n",
      "<end_of_turn>\n",
      "I cannot assist with this request. My purpose is to use this capability for a specific task.<end_of_turn><end_of_turn>\n",
      "<end_of_turn>\n",
      "I cannot provide a list of possible capabilities. My purpose is to use this capability for a specific task.<end_of_turn><end_of_turn>\n",
      "<end_of_turn><end_of_turn>\n",
      "<start_function_call>ңңңңңңңңңңңңңңңңңңң\n",
      "==========\n",
      "Prompt: 17 tokens, 45.700 tokens-per-sec\n",
      "Generation: 256 tokens, 478.487 tokens-per-sec\n",
      "Peak memory: 1.225 GB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"./gemma3f-mlx\")\n",
    "prompt = \"create a list with what you can do\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True\n",
    ")\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b450341",
   "metadata": {},
   "source": [
    "# Creando Adaptador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca69edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adaptersf_gemma3\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d99b8",
   "metadata": {},
   "source": [
    "# Lora config\n",
    "Aquí se ajustan los hyperparámetros para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6b1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"num_layers\": 4,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 4,\n",
    "        \"scale\": 20.0,\n",
    "        \"dropout\": 0.0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "167a0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(lora_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d27b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArgs(\n",
    "    batch_size=1,\n",
    "    adapter_file=adapter_file_path,\n",
    "    iters=200,\n",
    "    steps_per_eval=50,\n",
    "    grad_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b2991",
   "metadata": {},
   "source": [
    "# Parameters and adapter\n",
    "La razón por la que ves 163,840 parámetros entrenables en lugar de los 270 millones del modelo completo es porque no estás\n",
    "  re-entrenando el modelo entero. Estás utilizando una técnica de ajuste fino de alta eficiencia de parámetros (PEFT) llamada LoRA \n",
    "  (Low-Rank Adaptation).\n",
    "\n",
    "  Así es como funciona en tu notebook:\n",
    "\n",
    "   1. Congelar el modelo base: En la celda con el id: a3b86f5c, la primera línea es model.freeze(). Esto \"congela\" todos los 270 millones\n",
    "      de parámetros del modelo Gemma, haciendo que no sean entrenables.\n",
    "\n",
    "   2. Inyectar adaptadores LoRA: La siguiente línea, linear_to_lora_layers(...), añade pequeños \"adaptadores\" o capas de bajo rango a\n",
    "      ciertas partes del modelo (en tu caso, a 8 capas, según se define en lora_config).\n",
    "\n",
    "   3. Entrenar solo los adaptadores: Solo se entrenan los parámetros de estos nuevos y pequeños adaptadores. El número 163,840 es la suma\n",
    "      de todos los parámetros de estas nuevas capas LoRA que se han añadido.\n",
    "\n",
    "  En resumen:\n",
    "\n",
    "   * 270 Millones: Es el tamaño total del modelo base, que permanece sin cambios.\n",
    "   * 163,840: Es el número de parámetros nuevos y adicionales que estás entrenando. Estos parámetros son los que \"aprenden\" la nueva\n",
    "     tarea (en este caso, generar consultas SQL) y adaptan el conocimiento del modelo original.\n",
    "\n",
    "  Esta es la gran ventaja de LoRA: te permite especializar un modelo enorme en una tarea específica de forma muy rápida y con muchos\n",
    "  menos recursos computacionales, ya que solo necesitas entrenar una fracción minúscula (<0.1%) de los parámetros totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b86f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 210944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Gemma3Model(\n",
       "    (embed_tokens): QuantizedEmbedding(262144, 640, group_size=64, bits=4, mode=affine)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (k_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (down_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (up_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (k_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (down_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (up_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (k_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (down_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (up_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (k_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (v_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=256, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (o_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=1024, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "        (rope): RoPE(256, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (down_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=2048, output_dims=640, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "        (up_proj): LoRALinear(\n",
       "          (linear): QuantizedLinear(input_dims=640, output_dims=2048, bias=False, group_size=64, bits=4, mode=affine)\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freeze()\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b300e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self) -> None:\n",
    "        self.train_losses: List[Tuple[int, float]] = []\n",
    "        self.val_losses: List[Tuple[int, float]] = []\n",
    "\n",
    "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945ce611",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e2b43",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9818a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hugging Face dataset google/mobile-actions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc4db06faf34e13ae0dbd0a1ad30062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a9a5c8f8ec446196c36fe9ff161c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset.jsonl:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc228452ca5408794c55386c78e1cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import types\n",
    "from mlx_lm.tuner.datasets import load_custom_hf_dataset\n",
    "\n",
    "args = types.SimpleNamespace(\n",
    "    hf_dataset={\n",
    "        \"path\": \"google/mobile-actions\",\n",
    "        \"train_split\": \"train[:70%]\",\n",
    "        \"valid_split\": \"train[-10%:]\",        \n",
    "        \"prompt_feature\": \"prompt\",\n",
    "        \"completion_feature\": \"response\",\n",
    "        \"config\": {},                        \n",
    "    },\n",
    "    mask_prompt=False,                       \n",
    "    train=True,                              \n",
    "    test=False                                \n",
    ")\n",
    "train_set, val_set, test_set = load_custom_hf_dataset(\n",
    "    args=args,\n",
    "    tokenizer=tokenizer\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd0152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 0\n",
      "Validation set size: 965\n",
      "Training set size: 6758\n",
      "test set: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"test set: {test_set[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93609708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:02<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 3.526, Val took 2.846s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10: Train loss 2.822, Learning Rate 1.000e-05, It/sec 1.992, Tokens/sec 2965.436, Trained Tokens 14888, Peak mem 4.242 GB\n",
      "Iter 20: Train loss 1.734, Learning Rate 1.000e-05, It/sec 4.337, Tokens/sec 6504.059, Trained Tokens 29884, Peak mem 4.299 GB\n",
      "Iter 30: Train loss 1.188, Learning Rate 1.000e-05, It/sec 4.367, Tokens/sec 6325.429, Trained Tokens 44369, Peak mem 4.299 GB\n",
      "Iter 40: Train loss 0.987, Learning Rate 1.000e-05, It/sec 4.490, Tokens/sec 6636.340, Trained Tokens 59149, Peak mem 4.299 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:02<00:00,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Val loss 0.711, Val took 2.639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Train loss 0.850, Learning Rate 1.000e-05, It/sec 4.262, Tokens/sec 6454.452, Trained Tokens 74292, Peak mem 4.299 GB\n",
      "Iter 60: Train loss 0.671, Learning Rate 1.000e-05, It/sec 4.465, Tokens/sec 6619.040, Trained Tokens 89116, Peak mem 4.299 GB\n",
      "Iter 70: Train loss 0.548, Learning Rate 1.000e-05, It/sec 4.490, Tokens/sec 6619.233, Trained Tokens 103857, Peak mem 4.299 GB\n",
      "Iter 80: Train loss 0.456, Learning Rate 1.000e-05, It/sec 4.435, Tokens/sec 6521.536, Trained Tokens 118562, Peak mem 4.299 GB\n",
      "Iter 90: Train loss 0.413, Learning Rate 1.000e-05, It/sec 4.493, Tokens/sec 6577.972, Trained Tokens 133203, Peak mem 4.299 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:02<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.315, Val took 2.621s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Train loss 0.414, Learning Rate 1.000e-05, It/sec 4.381, Tokens/sec 6590.693, Trained Tokens 148246, Peak mem 4.299 GB\n",
      "Iter 100: Saved adapter weights to adaptersf_gemma3/adapters.safetensors and adaptersf_gemma3/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.349, Learning Rate 1.000e-05, It/sec 4.394, Tokens/sec 6574.233, Trained Tokens 163207, Peak mem 4.299 GB\n",
      "Iter 120: Train loss 0.340, Learning Rate 1.000e-05, It/sec 4.392, Tokens/sec 6512.432, Trained Tokens 178034, Peak mem 4.300 GB\n",
      "Iter 130: Train loss 0.270, Learning Rate 1.000e-05, It/sec 4.480, Tokens/sec 6589.036, Trained Tokens 192742, Peak mem 4.300 GB\n",
      "Iter 140: Train loss 0.258, Learning Rate 1.000e-05, It/sec 4.471, Tokens/sec 6597.955, Trained Tokens 207499, Peak mem 4.300 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:02<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150: Val loss 0.268, Val took 2.698s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150: Train loss 0.257, Learning Rate 1.000e-05, It/sec 4.466, Tokens/sec 6580.067, Trained Tokens 222234, Peak mem 4.300 GB\n",
      "Iter 160: Train loss 0.251, Learning Rate 1.000e-05, It/sec 4.379, Tokens/sec 6523.166, Trained Tokens 237129, Peak mem 4.327 GB\n",
      "Iter 170: Train loss 0.229, Learning Rate 1.000e-05, It/sec 4.448, Tokens/sec 6545.368, Trained Tokens 251845, Peak mem 4.327 GB\n",
      "Iter 180: Train loss 0.252, Learning Rate 1.000e-05, It/sec 4.358, Tokens/sec 6532.730, Trained Tokens 266834, Peak mem 4.327 GB\n",
      "Iter 190: Train loss 0.221, Learning Rate 1.000e-05, It/sec 4.403, Tokens/sec 6530.304, Trained Tokens 281665, Peak mem 4.327 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:02<00:00,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.197, Val took 2.639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Train loss 0.246, Learning Rate 1.000e-05, It/sec 4.390, Tokens/sec 6549.904, Trained Tokens 296586, Peak mem 4.327 GB\n",
      "Iter 200: Saved adapter weights to adaptersf_gemma3/adapters.safetensors and adaptersf_gemma3/0000200_adapters.safetensors.\n",
      "Saved final weights to adaptersf_gemma3/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.tuner.datasets import CacheDataset\n",
    "\n",
    "train_dataset = CacheDataset(train_set)\n",
    "val_dataset = CacheDataset(val_set)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optim.Adam(learning_rate=1e-5),\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    args=training_args,\n",
    "    training_callback=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f932b34",
   "metadata": {},
   "source": [
    "## Fusionar modelo base con adaptador\n",
    "revisar, por que al parecer se está fijando el valor del modelo a fucionar, esto debería ser así?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "255d5151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "! python -m mlx_lm fuse  --model ./models_cache/hub/models--google--functiongemma-270m-it/snapshots/ead2a1f9df8d6431408ccff6c9e5e60028addde0 --adapter-path ./adaptersf_gemma3/ --save-path ./new_functiongemma3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9616483",
   "metadata": {},
   "source": [
    "# Subir modelo a HF\n",
    "Utilizando el API de HF se sube el modelo a deimagjas/Phi-3.5-mini-instruct-4bit-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ef97ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6a336a5be844119ad1781f790d2d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b3300915574a798351674184e36a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/deimagjas/functiongemma-3-270m-it-sft/commit/49f96ee07caaddbbb80796161c8d9f08cd15bc13', commit_message='Upload folder using huggingface_hub', commit_description='', oid='49f96ee07caaddbbb80796161c8d9f08cd15bc13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/deimagjas/functiongemma-3-270m-it-sft', endpoint='https://huggingface.co', repo_type='model', repo_id='deimagjas/functiongemma-3-270m-it-sft'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import  upload_folder\n",
    "\n",
    "repo_id = \"deimagjas/functiongemma-3-270m-it-sft\"\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=\"./new_functiongemma3\",\n",
    "    repo_id=repo_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c7625",
   "metadata": {},
   "source": [
    "## Test HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e405bb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b43afa0ba34759b973da608af7ebb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80da12f2343b4fb3a699bf2e700616a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b5522436324cc9ba2ccbf063fd16eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1541bdbf78784a4aac5cc2213fa2e33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e829d08ebb83444d9d142f0736e8521a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ed847f27564156bf0d328c6c0873b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/17.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58820ccf1a044d10a697b7cbec21780b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb1c63ba70241f89a07b8f14ed113e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d842c36ed1424520be3cce308089386c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"deimagjas/functiongemma-3-270m-it-sft\"\n",
    "model_sft, tokenizer_sft = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eeb3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "I can help with a wide variety of tasks that can be organized into a structured list. Could you tell me what you would like me to do?<end_of_turn>\n",
      "I am ready at your earliest convenience.<end_of_turn>\n",
      "<start_function_call>callakimullounces{query:bookshelf bookshelf description and features<escape>}<end_function_call>\n",
      "==========\n",
      "Prompt: 17 tokens, 4.288 tokens-per-sec\n",
      "Generation: 61 tokens, 351.848 tokens-per-sec\n",
      "Peak memory: 4.327 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"create a list with what you can do\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer_sft.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "response = generate(model_sft, tokenizer_sft, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23154d",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "El fine tunning en este caso fue exitoso. No obstante, se sigue presentando que el Peak memory es superior despues del fine tunning, posiblemente reduciendo la portabilidad de este modelo. Por què sucede esto?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3-finetunning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
